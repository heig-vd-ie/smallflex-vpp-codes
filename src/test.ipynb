{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliary.download_data import download_from_switch\n",
    "from auxiliary.read_gries_data import read_gries_txt_data\n",
    "from auxiliary.read_gletsch_data import read_gletsch_csv_data\n",
    "from auxiliary.read_swissgrid_data import read_spot_price_swissgrid, read_balancing_price_swissgrid, read_fcr_price_swissgrid, read_frr_price_swissgrid\n",
    "from auxiliary.auxiliary import read_pyarrow_data\n",
    "import polars as pl\n",
    "import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "from schema.schema import Irradiation, WindSpeed, Temperature, DischargeFlow, MarketPrice\n",
    "from federation.generate_forecast import generate_dataframe_forecast\n",
    "from sqlalchemy import create_engine\n",
    "from schema.schema import Base\n",
    "from uuid import uuid4\n",
    "import polars as pl\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tqdm\n",
    "import polars.selectors as cs\n",
    "# from federation.generate_scenarios import initialize_time_series, generate_scenarios, query_time_series_data, fill_null_remove_outliers\n",
    "from schema.schema import (\n",
    "    DischargeFlow,\n",
    "    Irradiation,\n",
    "    WindSpeed,\n",
    "    Temperature,\n",
    "    MarketPrice,\n",
    ")\n",
    "import polars as pl\n",
    "import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "from schema.schema import Irradiation, WindSpeed, Temperature, DischargeFlow, MarketPrice\n",
    "from federation_2.generate_scenarios import generate_scenarios, fill_null_remove_outliers\n",
    "from federation_2.generate_forecast import day_ahead_forecast_arima_with_lag\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "import pmdarima as pm\n",
    "import polars as pl\n",
    "from pmdarima.pipeline import Pipeline\n",
    "# from federation.generate_forecast import generate_dataframe_forecast\n",
    "from datetime import timedelta\n",
    "from auxiliary.plot_results import plot_forecast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read MarketPrice table in sqlite database: 100%|██████████████████████| 1/1 [00:05<00:00,  5.46s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialize price signals\n",
    "\"\"\"\n",
    "db_cache_file = r\".cache/interim/time_series_schema.db\"\n",
    "table = MarketPrice\n",
    "\n",
    "\n",
    "engine = create_engine(f\"sqlite+pysqlite:///{db_cache_file}\", echo=False)\n",
    "con = engine.connect()\n",
    "\n",
    "d_time=timedelta(hours=1)\n",
    "total_data_forecast_df: pl.DataFrame = pl.DataFrame()\n",
    "\n",
    "table_name = table.__tablename__\n",
    "if table_name == \"MarketPrice\":\n",
    "    type_col = \"market\"\n",
    "elif table_name == \"DischargeFlow\":\n",
    "    type_col = \"river\"\n",
    "else:\n",
    "    type_col = \"alt\"\n",
    "non_negative = table_name in [\"Irradiation\", \"WindSpeed\", \"river\"] \n",
    "z_score = 4 if table_name in [\"Irradiation\", \"Temperature\"] else 10\n",
    "\n",
    "with tqdm.tqdm(total=1, ncols=100, desc=\"Read {} table in sqlite database\".format(table_name)) as pbar: \n",
    "    raw_data_df = pl.read_database(query=\"\"\"SELECT {f1}.* FROM {f1}\"\"\".format(f1=table_name), connection=con)  \n",
    "    pbar.update()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for type_value in tqdm.tqdm(raw_data_df[type_col].unique(), desc=\"Clean data and generate scenarios of \" + table_name):\n",
    "type_value =\"DA price\"\n",
    "cleaned_data_df: pl.DataFrame = fill_null_remove_outliers(\n",
    "    raw_data_df.filter(pl.col(type_col) == type_value), d_time=d_time, z_score=z_score\n",
    ")\n",
    "data_scenarios_df, fill_null= generate_scenarios(cleaned_data_df, d_time=d_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_forecast_df: pl.DataFrame = day_ahead_forecast_arima_with_lag(\n",
    "    data_scenarios_df, d_time=d_time, non_negative=non_negative\n",
    ")\n",
    "plot_forecast_results(data_scenarios_df=data_scenarios_df, data_forecast_df=data_forecast_df, file_name=table_name + \"_\" + str(type_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (76_968, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>year</th><th>week</th><th>time_step</th><th>value</th><th>delta_t</th></tr><tr><td>datetime[μs]</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>duration[μs]</td></tr></thead><tbody><tr><td>2015-01-01 00:00:00</td><td>2015</td><td>1</td><td>73</td><td>44.94</td><td>1h</td></tr><tr><td>2015-01-01 01:00:00</td><td>2015</td><td>1</td><td>74</td><td>43.43</td><td>1h</td></tr><tr><td>2015-01-01 02:00:00</td><td>2015</td><td>1</td><td>75</td><td>38.08</td><td>1h</td></tr><tr><td>2015-01-01 03:00:00</td><td>2015</td><td>1</td><td>76</td><td>35.47</td><td>1h</td></tr><tr><td>2015-01-01 04:00:00</td><td>2015</td><td>1</td><td>77</td><td>30.83</td><td>1h</td></tr><tr><td>2015-01-01 05:00:00</td><td>2015</td><td>1</td><td>78</td><td>28.26</td><td>1h</td></tr><tr><td>2015-01-01 06:00:00</td><td>2015</td><td>1</td><td>79</td><td>25.36</td><td>1h</td></tr><tr><td>2015-01-01 07:00:00</td><td>2015</td><td>1</td><td>80</td><td>26.93</td><td>1h</td></tr><tr><td>2015-01-01 08:00:00</td><td>2015</td><td>1</td><td>81</td><td>24.95</td><td>1h</td></tr><tr><td>2015-01-01 09:00:00</td><td>2015</td><td>1</td><td>82</td><td>26.71</td><td>1h</td></tr><tr><td>2015-01-01 10:00:00</td><td>2015</td><td>1</td><td>83</td><td>30.04</td><td>1h</td></tr><tr><td>2015-01-01 11:00:00</td><td>2015</td><td>1</td><td>84</td><td>31.77</td><td>1h</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2023-10-26 12:00:00</td><td>2023</td><td>43</td><td>85</td><td>126.12</td><td>1h</td></tr><tr><td>2023-10-26 13:00:00</td><td>2023</td><td>43</td><td>86</td><td>123.1</td><td>1h</td></tr><tr><td>2023-10-26 14:00:00</td><td>2023</td><td>43</td><td>87</td><td>123.57</td><td>1h</td></tr><tr><td>2023-10-26 15:00:00</td><td>2023</td><td>43</td><td>88</td><td>130.65</td><td>1h</td></tr><tr><td>2023-10-26 16:00:00</td><td>2023</td><td>43</td><td>89</td><td>133.87</td><td>1h</td></tr><tr><td>2023-10-26 17:00:00</td><td>2023</td><td>43</td><td>90</td><td>143.9</td><td>1h</td></tr><tr><td>2023-10-26 18:00:00</td><td>2023</td><td>43</td><td>91</td><td>152.49</td><td>1h</td></tr><tr><td>2023-10-26 19:00:00</td><td>2023</td><td>43</td><td>92</td><td>153.9</td><td>1h</td></tr><tr><td>2023-10-26 20:00:00</td><td>2023</td><td>43</td><td>93</td><td>132.37</td><td>1h</td></tr><tr><td>2023-10-26 21:00:00</td><td>2023</td><td>43</td><td>94</td><td>120.46</td><td>1h</td></tr><tr><td>2023-10-26 22:00:00</td><td>2023</td><td>43</td><td>95</td><td>116.32</td><td>1h</td></tr><tr><td>2023-10-26 23:00:00</td><td>2023</td><td>43</td><td>96</td><td>107.86</td><td>1h</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (76_968, 6)\n",
       "┌─────────────────────┬──────┬──────┬───────────┬────────┬──────────────┐\n",
       "│ timestamp           ┆ year ┆ week ┆ time_step ┆ value  ┆ delta_t      │\n",
       "│ ---                 ┆ ---  ┆ ---  ┆ ---       ┆ ---    ┆ ---          │\n",
       "│ datetime[μs]        ┆ i64  ┆ i64  ┆ i64       ┆ f64    ┆ duration[μs] │\n",
       "╞═════════════════════╪══════╪══════╪═══════════╪════════╪══════════════╡\n",
       "│ 2015-01-01 00:00:00 ┆ 2015 ┆ 1    ┆ 73        ┆ 44.94  ┆ 1h           │\n",
       "│ 2015-01-01 01:00:00 ┆ 2015 ┆ 1    ┆ 74        ┆ 43.43  ┆ 1h           │\n",
       "│ 2015-01-01 02:00:00 ┆ 2015 ┆ 1    ┆ 75        ┆ 38.08  ┆ 1h           │\n",
       "│ 2015-01-01 03:00:00 ┆ 2015 ┆ 1    ┆ 76        ┆ 35.47  ┆ 1h           │\n",
       "│ 2015-01-01 04:00:00 ┆ 2015 ┆ 1    ┆ 77        ┆ 30.83  ┆ 1h           │\n",
       "│ …                   ┆ …    ┆ …    ┆ …         ┆ …      ┆ …            │\n",
       "│ 2023-10-26 19:00:00 ┆ 2023 ┆ 43   ┆ 92        ┆ 153.9  ┆ 1h           │\n",
       "│ 2023-10-26 20:00:00 ┆ 2023 ┆ 43   ┆ 93        ┆ 132.37 ┆ 1h           │\n",
       "│ 2023-10-26 21:00:00 ┆ 2023 ┆ 43   ┆ 94        ┆ 120.46 ┆ 1h           │\n",
       "│ 2023-10-26 22:00:00 ┆ 2023 ┆ 43   ┆ 95        ┆ 116.32 ┆ 1h           │\n",
       "│ 2023-10-26 23:00:00 ┆ 2023 ┆ 43   ┆ 96        ┆ 107.86 ┆ 1h           │\n",
       "└─────────────────────┴──────┴──────┴───────────┴────────┴──────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_value = \"FCR\"\n",
    "time_step_per_day: int = int(timedelta(days=1)/d_time)\n",
    "raw_data_df = raw_data_df.filter(pl.col(type_col) == type_value)\n",
    "cleaned_data_df = raw_data_df.select([\n",
    "    pl.col(\"timestamp\").str.strptime(dtype=pl.Datetime, format=\"%Y-%m-%d %H:%M:%S%.6f\").dt.truncate(d_time),\n",
    "    pl.col(\"value\")   \n",
    "]).sort(\"timestamp\")\\\n",
    ".group_by(\"timestamp\", maintain_order=True).agg(pl.col(\"value\").mean())\n",
    "\n",
    "# Remove outlier using z_score test\n",
    "cleaned_data_df = cleaned_data_df\\\n",
    ".with_columns(\n",
    "    (((pl.col(\"value\") - pl.col(\"value\").mean()) / pl.col(\"value\").std()).abs()).alias(\"z_score\")\n",
    ").filter(pl.col(\"z_score\") < z_score)\n",
    "\n",
    "# fill missing values with data of previous, next day and week\n",
    "cleaned_data_df = cleaned_data_df.sort(\"timestamp\")\\\n",
    ".upsample(time_column=\"timestamp\", every=d_time, maintain_order=True)\\\n",
    ".with_columns(\n",
    "    pl.col(\"value\").fill_null(pl.col(\"value\").shift(n = time_step_per_day))\n",
    "    .fill_null(pl.col(\"value\").shift(n = -time_step_per_day))\n",
    "    .fill_null(pl.col(\"value\").shift(n = 7*time_step_per_day))\n",
    "    .fill_null(pl.col(\"value\").shift(n = -7*time_step_per_day))\n",
    "    .fill_null(strategy=\"forward\"),\n",
    "    pl.col(\"timestamp\").dt.year().cast(pl.Int64).alias(\"year\"),\n",
    "    pl.col(\"timestamp\").dt.week().cast(pl.Int64).alias(\"week\"),  \n",
    "    pl.col(\"timestamp\").dt.weekday().cast(pl.Int64).alias(\"weekday\"),    \n",
    "    pl.col(\"timestamp\").dt.month().cast(pl.Int64).alias(\"month\"),  \n",
    ")\n",
    "\n",
    "# Remove 53 week if exist and change year of first and last week in order to have consistent year \n",
    "if cleaned_data_df[\"year\"].unique().shape[0] != 1:\n",
    "    cleaned_data_df = cleaned_data_df\\\n",
    "    .filter(pl.col(\"week\") <= 52)\\\n",
    "    .with_columns(\n",
    "        pl.when((pl.col(\"week\") == 52) & (pl.col(\"month\") == 1))\n",
    "        .then(pl.col(\"year\") - 1)\n",
    "        .when((pl.col(\"week\") == 1) & (pl.col(\"month\") == 12))\n",
    "        .then(pl.col(\"year\") + 1)\n",
    "        .otherwise(pl.col(\"year\")).alias(\"year\")\n",
    "    )\n",
    "else:\n",
    "    cleaned_data_df = cleaned_data_df.sort([\"year\",\"week\",  \"weekday\",\"timestamp\"])\n",
    "# define time step\n",
    "cleaned_data_df = cleaned_data_df.with_columns([\n",
    "cleaned_data_df.group_by(['year', 'week', 'weekday'], maintain_order=True)\\\n",
    ".agg(pl.int_range(0, pl.len()).alias(\"time_step\"))\\\n",
    ".explode(\"time_step\")[\"time_step\"]\n",
    "]).with_columns(\n",
    "    (pl.col(\"time_step\") + (pl.col(\"weekday\") - 1)*time_step_per_day).alias(\"time_step\")\n",
    ").filter(pl.struct([\"year\",\"week\", \"time_step\"]).is_first_distinct()).sort([\"year\",\"week\", \"time_step\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_data_df = cleaned_data_df\\\n",
    ".select([\n",
    "    pl.col(\"timestamp\"), \n",
    "    pl.col([\"year\", \"week\", \"time_step\"]).cast(pl.Int64),\n",
    "    pl.col(\"value\"), \n",
    "    pl.lit(d_time).alias(\"delta_t\")\n",
    "]).sort([\"year\", \"week\", \"time_step\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_scenarios_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m raw_data_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_scenarios_dict\u001b[49m[d] \n\u001b[1;32m      2\u001b[0m scenario_list \u001b[38;5;241m=\u001b[39m raw_data_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscenario\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m      4\u001b[0m raw_data_df \u001b[38;5;241m=\u001b[39m raw_data_df\u001b[38;5;241m.\u001b[39mpivot(values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweek\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_step\u001b[39m\u001b[38;5;124m\"\u001b[39m], columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscenario\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msort([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweek\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_step\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_scenarios_dict' is not defined"
     ]
    }
   ],
   "source": [
    "raw_data_df = data_scenarios_dict[d] \n",
    "scenario_list = raw_data_df[\"scenario\"].unique().to_list()\n",
    "\n",
    "raw_data_df = raw_data_df.pivot(values=\"value\", index=[\"week\", \"time_step\"], columns=\"scenario\").sort([\"week\", \"time_step\"])\n",
    "\n",
    "predicted_data_df = raw_data_df.with_columns([\n",
    "    predict_years(values=raw_data_df[scenario], step_per_day=step_per_day, days_to_predict=days_to_predict, lag=4).alias(scenario)\n",
    "    for scenario in scenario_list\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_data_df = cleaned_data_df\\\n",
    ".select([\n",
    "    pl.col(\"timestamp\"), \n",
    "    pl.col([\"year\", \"week\", \"time_step\"]).cast(pl.Int64),\n",
    "    pl.col(\"value\"), \n",
    "    pl.lit(d_time).alias(\"delta_t\")\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, values in data.items():\n",
    "#     data[key] = data[key].with_columns(pl.col(\"week\").cast(pl.UInt32))\n",
    "\n",
    "\n",
    "\n",
    "    # data_forecast[d] = generate_dataframe_forecast(data_scenarios[d], d_time=\"1h\", non_negative=non_negative)\n",
    "#     data_scenarios[d] = data_scenarios[d].select([\"week\", \"time_step\", \"scenario\", \"delta_t\", \"value\"]).with_columns([pl.lit(d).alias(additional_column), pl.lit(\"RT\").alias(\"horizon\")])\n",
    "#     data_forecast[d] = data_forecast[d].select([\"week\", \"time_step\", \"scenario\", \"delta_t\", \"value\"]).with_columns([pl.lit(d).alias(additional_column), pl.lit(\"DA\").alias(\"horizon\")])\n",
    "#     result_data = pl.concat([result_data, data_scenarios[d], data_forecast[d]])\n",
    "# result_data.write_database(table_name=table_schema_name + \"Norm\", connection=f\"sqlite+pysqlite:///{db_cache_file}\", if_exists=\"replace\", engine=\"sqlalchemy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_time=timedelta(hours=1)\n",
    "z_score=10\n",
    "cleaned_data_df = data[d].select([\n",
    "    pl.col(\"timestamp\").str.strptime(dtype=pl.Datetime, format=\"%Y-%m-%d %H:%M:%S%.6f\").dt.truncate(d_time),\n",
    "    pl.col(\"value\")   \n",
    "]).group_by(\"timestamp\", maintain_order=True).agg(pl.col(\"value\").mean())\n",
    "\n",
    "# Remove outlier using z_score test\n",
    "cleaned_data_df = cleaned_data_df\\\n",
    ".with_columns(\n",
    "    (((pl.col(\"value\") - pl.col(\"value\").mean()) / pl.col(\"value\").std()).abs()).alias(\"z_score\")\n",
    ").filter(pl.col(\"z_score\") < z_score)\n",
    "\n",
    "# fill missing values with data of previous, next day and week\n",
    "cleaned_data_df = cleaned_data_df.sort(\"timestamp\")\\\n",
    ".upsample(time_column=\"timestamp\", every=d_time, maintain_order=True)\\\n",
    ".with_columns(\n",
    "    pl.col(\"value\").fill_null(pl.col(\"value\").shift(n = int(timedelta(days=1)/d_time)))\n",
    "    .fill_null(pl.col(\"value\").shift(n = -int(timedelta(days=1)/d_time)))\n",
    "    .fill_null(pl.col(\"value\").shift(n = int(timedelta(days=7)/d_time)))\n",
    "    .fill_null(pl.col(\"value\").shift(n = -int(timedelta(days=7)/d_time)))\n",
    "    .fill_null(strategy=\"forward\"),\n",
    "    pl.col(\"timestamp\").dt.year().alias(\"year\"),\n",
    "    pl.col(\"timestamp\").dt.week().alias(\"week\"),  \n",
    "    (pl.col(\"timestamp\").dt.weekday() - 1).alias(\"weekday\"),    \n",
    "    pl.col(\"timestamp\").dt.month().alias(\"month\"),  \n",
    ")\n",
    "# Remove 53 week if exist and change \n",
    "cleaned_data_df = cleaned_data_df\\\n",
    ".filter(pl.col(\"week\") <= 52)\\\n",
    ".with_columns(\n",
    "    pl.when((pl.col(\"week\") == 52) & (pl.col(\"month\") == 1))\n",
    "    .then(pl.col(\"year\") - 1)\n",
    "    .when((pl.col(\"week\") == 1) & (pl.col(\"month\") == 12))\n",
    "    .then(pl.col(\"year\") + 1)\n",
    "    .otherwise(pl.col(\"year\")).alias(\"year\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers_df = pl.read_database(query=\"\"\"SELECT {f1}.* FROM {f1}\"\"\".format(f1=DischargeFlow.__tablename__), connection=con)\n",
    "rivers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_mean_val = cleaned_data[\"Short price\"].group_by(\"year\").agg(pl.col(\"value\").mean()).sort(\"value\")\n",
    "year_median = yearly_mean_val.with_columns((pl.col(\"value\")-pl.mean(\"value\"))**2).sort(\"value\")[\"year\"][0]\n",
    "year_mapping_df = year_mapping_df = pl.DataFrame({\n",
    "    \"year\": [yearly_mean_val[\"year\"][0], yearly_mean_val[\"year\"][-1], year_median],\n",
    "    \"scenario\" : [\"min\", \"max\", \"median\"]\n",
    "}).with_columns(pl.col(\"year\").cast(pl.Int64))\n",
    "year_mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stat_data_by_year = cleaned_data[\"Short price\"].join(year_mapping_df, on=\"year\", how=\"inner\")\n",
    "    \n",
    "stat_data_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time_step : find min max and median for each time step\n",
    "stat_data_by_time_step =cleaned_data[\"Short price\"].group_by([\"week\", \"time_step\"], maintain_order=True).agg([\n",
    "        pl.col(\"value\").mean().alias(\"median\"),\n",
    "        pl.col(\"value\").max().alias(\"max\"),\n",
    "        pl.col(\"value\").min().alias(\"min\"),\n",
    "        pl.col(\"delta_t\").first()\n",
    "    ]).melt(\n",
    "        id_vars =[\"week\", \"time_step\", \"delta_t\"], value_name=\"time_step_value\",\n",
    "        value_vars=[\"median\",\"max\",\"min\"], variable_name=\"scenario\"\n",
    "    ).sort([\"week\", \"time_step\"])\n",
    "stat_data_by_time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scenario using yearly stat data with null filled with main set stat\n",
    "scenario_data_df = stat_data_by_time_step.join(stat_data_by_year, on=[\"week\", \"time_step\", \"scenario\"], how=\"outer\")\\\n",
    "    .select([\n",
    "        pl.col([\"week\", \"time_step\", \"delta_t\", \"scenario\"]),\n",
    "        pl.col(\"value\").fill_null(pl.col(\"time_step_value\")).alias(\"value\")\n",
    "    ])\n",
    "scenario_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in cleaned_data.values():\n",
    "    display(df.group_by(\"year\", maintain_order=True).agg(pl.col(\"value\").count()))\n",
    "    display(df.filter(pl.col(\"year\") == 2015).group_by(\"week\", maintain_order=True).agg(pl.col(\"value\").count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_df = cleaned_data_df.upsample(time_column=\"timestamp\", every=timedelta(minutes=15), maintain_order=True)\\\n",
    ".with_columns(\n",
    "    pl.col(\"value\").fill_null(pl.col(\"value\").shift(n = int(timedelta(days=1)/d_time)))\n",
    "    .fill_null(pl.col(\"value\").shift(n = -int(timedelta(days=1)/d_time))),\n",
    "    pl.col(\"timestamp\").dt.year().alias(\"year\"),\n",
    "    pl.col(\"timestamp\").dt.week().alias(\"week\"),   \n",
    ")\n",
    "print(cleaned_data_df.null_count())\n",
    "cleaned_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_data_df = data[\"Long price\"].select([\"timestamp\", \"value\"])\n",
    "avg = raw_data_df.mean().get_column(\"value\")[0]\n",
    "std = raw_data_df.std().get_column(\"value\")[0]\n",
    "raw_data_df = raw_data_df.with_columns(((pl.col(\"value\") - avg) / std).alias(\"z_score\"))\n",
    "raw_data_df = raw_data_df.filter((pl.col(\"z_score\") < z_score) & (pl.col(\"z_score\") > -z_score))\n",
    "# fill missing values with data of previous day\n",
    "raw_data_df = raw_data_df.with_columns(pl.col(\"timestamp\").str.strptime(dtype=pl.Datetime, format=\"%Y-%m-%d %H:%M:%S%.6f\")).set_sorted(\"timestamp\", descending=False).upsample(time_column=\"timestamp\", every=d_time)\n",
    "raw_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_scenarios[d]\n",
    "d_time=\"1h\"\n",
    "non_negative=non_negative    \n",
    "d_time_int = int(d_time.split(\"h\")[0]) if \"h\" in d_time else int(d_time.split(\"m\")[0]) / 60 if \"m\" in d_time else RuntimeError\n",
    "def generate_arbitrary_year(year_col: pl.col) -> pl.col:\n",
    "    return year_col.dt.strftime(\"2030-%m-%d %H:%M:%S\")\\\n",
    "        .str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\", strict=False).alias(\"arbitrary_year\")\n",
    "\n",
    "# result_df = pl.DataFrame(schema={\"timestamp\": pl.Datetime(time_unit=\"ns\"), \"value\": pl.Float64, \"scenario\": pl.Utf8})\n",
    "data_df.with_columns(generate_arbitrary_year(pl.col(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scen in data_df[\"scenario\"].unique().to_list():\n",
    "    df_temp = data_df.filter(pl.col(\"scenario\")==scen).with_columns(arbitrary_year).sort(\"timestamp\").select([\"timestamp\", \"value\"]).to_pandas().set_index(\"timestamp\", drop=True)\n",
    "    df_temp = df_temp[~df_temp.index.duplicated()].asfreq('1h', method = 'ffill')\n",
    "    result_temp = day_ahead_forecast_arima_with_lag(df_temp, non_negative=non_negative)\n",
    "    data_forecast_temp = pl.from_dict({\"timestamp\": df_temp.index, \"value\": result_temp}).with_columns(pl.lit(scen).alias(\"scenario\"))\n",
    "    print(result_df.head())\n",
    "    print(data_forecast_temp.head())\n",
    "    result_df = pl.concat([result_df, data_forecast_temp])\n",
    "result_df = result_df.with_columns([arbitrary_year.dt.week().alias(\"week\"), pl.col(\"timestamp\").dt.year().alias(\"year\")])\n",
    "# define time step\n",
    "result_df = result_df.group_by([\"week\", \"year\", \"scenario\"], maintain_order=True).agg([pl.col(\"timestamp\"), pl.col(\"value\")]).with_columns(pl.col(\"value\").map_elements(lambda x: range(len(x))).alias(\"time_step\"))\n",
    "result_df = result_df.explode([\"timestamp\", \"time_step\", \"value\"]).select([\"timestamp\", \"year\", \"week\", \"time_step\", \"value\", \"scenario\"])\n",
    "# remove first and end weeks to have consistent years\n",
    "result_df = result_df.filter((pl.col(\"week\") < 53) & (pl.col(\"time_step\") < int(168 / d_time_int))).with_columns(pl.lit(d_time_int).alias(\"delta_t\").cast(pl.Float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scenarios[d]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smallflex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
